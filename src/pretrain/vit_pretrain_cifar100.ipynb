{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm tqdm matplotlib\n",
    "\n",
    "# Cell 2: Mount Google Drive for saving models and checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for checkpoints\n",
    "!mkdir -p /content/drive/MyDrive/vit_pretrained_cifar100/checkpoints/cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Import libraries and set up device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # Use notebook version for better Colab progress bars\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# For ViT model\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False  # Disable benchmarking for reproducibility\n",
    "    # Set seeds for data loading operations\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # Document the environment for future reproducibility\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Worker initialization function for DataLoaders\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Set device - Colab typically provides a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this class after your TinyImageNet class definition\n",
    "class ClassRemappingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, class_mapping):\n",
    "        self.dataset = dataset\n",
    "        self.class_mapping = class_mapping\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[index]\n",
    "        # Map the original class index to the new consecutive index\n",
    "        new_target = self.class_mapping[target]\n",
    "        return img, new_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset preparation function - updated to track left-out examples\n",
    "def prepare_dataset(dataset_name, data_dir='/content/drive/MyDrive/data'):\n",
    "    \"\"\"\n",
    "    Prepare dataset for pretraining according to requirements:\n",
    "    - 80% of classes for pretraining\n",
    "    - 75% of each pretraining class examples\n",
    "    - 20% of classes reserved for continual learning\n",
    "    Now also tracks the 25% left-out examples from pretraining classes for continual learning\n",
    "    \"\"\"\n",
    "    if dataset_name == 'cifar100':\n",
    "        # Define transforms with stronger augmentation for training from scratch\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandAugment(num_ops=2, magnitude=9),  # More aggressive augmentation\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "            transforms.Resize((224, 224))  # Resize to ViT input size\n",
    "        ])\n",
    "\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "            transforms.Resize((224, 224))  # Resize to ViT input size\n",
    "        ])\n",
    "\n",
    "        # Load CIFAR-100 dataset\n",
    "        train_dataset = CIFAR100(root=data_dir, train=True, download=True, transform=train_transform)\n",
    "        test_dataset = CIFAR100(root=data_dir, train=False, download=True, transform=test_transform)\n",
    "\n",
    "        # Number of classes\n",
    "        n_classes = 100\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported\")\n",
    "\n",
    "    # The rest of the function continues as before\n",
    "    # Create subset datasets\n",
    "    print(\"Creating subset datasets...\")\n",
    "    pretrain_subset = Subset(train_dataset, pretrain_indices)\n",
    "    pretrain_dataset = ClassRemappingDataset(pretrain_subset, class_mapping)\n",
    "\n",
    "    # Create train-val split\n",
    "    n_pretrain = len(pretrain_dataset)\n",
    "    n_val = int(0.2 * n_pretrain)\n",
    "    n_train = n_pretrain - n_val\n",
    "\n",
    "    # Use generator for reproducible split\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    pretrain_train_dataset, pretrain_val_dataset = random_split(\n",
    "        pretrain_dataset, [n_train, n_val], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    batch_size = 32\n",
    "\n",
    "    pretrain_loader = DataLoader(\n",
    "        pretrain_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        pretrain_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    full_test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    print(f\"Pretraining on {len(pretrain_train_dataset)} samples\")\n",
    "    print(f\"Validation on {len(pretrain_val_dataset)} samples\")\n",
    "    print(f\"Full test set has {len(test_dataset)} samples\")\n",
    "    print(f\"Left out {len(left_out_indices)} examples from pretraining classes for continual learning\")\n",
    "\n",
    "    # Store class information\n",
    "    class_info = {\n",
    "        'n_classes': n_classes,\n",
    "        'pretrain_classes': pretrain_classes,\n",
    "        'continual_classes': continual_classes,\n",
    "        'left_out_indices': left_out_indices,\n",
    "        'class_mapping': class_mapping\n",
    "    }\n",
    "\n",
    "    return pretrain_loader, val_loader, full_test_loader, class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model creation and learning rate scheduler\n",
    "def create_vit_model_from_scratch(num_classes):\n",
    "    \"\"\"\n",
    "    Create a ViT model from scratch (without pre-trained weights)\n",
    "    \"\"\"\n",
    "    # Create ViT model with random initialization (pretrained=False)\n",
    "    model = timm.create_model('vit_small_patch16_224', pretrained=False, num_classes=num_classes, drop_path_rate=0.1)\n",
    "\n",
    "    # Better initialization for Transformers\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    model.apply(_init_weights)\n",
    "    return model\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "class WarmupCosineScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, max_epochs, warmup_start_lr=1e-6, eta_min=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.eta_min = eta_min\n",
    "\n",
    "        # Get base lr\n",
    "        self.base_lr = []\n",
    "        for group in optimizer.param_groups:\n",
    "            self.base_lr.append(group['lr'])\n",
    "\n",
    "    def step(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr_mult = epoch / self.warmup_epochs\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                group['lr'] = self.warmup_start_lr + lr_mult * (self.base_lr[i] - self.warmup_start_lr)\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                progress = (epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)\n",
    "                cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "                group['lr'] = self.eta_min + cosine_decay * (self.base_lr[i] - self.eta_min)\n",
    "\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MixUp and CutMix data augmentation\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_from_scratch(model, train_loader, val_loader, class_info, dataset_name,\n",
    "                           num_epochs=1):  # Reduced epochs for Colab\n",
    "    \"\"\"\n",
    "    Train the ViT model from scratch with proper hyperparameters and mixed precision\n",
    "    \"\"\"\n",
    "    # Import for mixed precision training\n",
    "    from torch.amp.autocast_mode import autocast\n",
    "    from torch.amp.grad_scaler import GradScaler\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # Higher learning rate for training from scratch\n",
    "    # Weight decay is important for regularization when training from scratch\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05, betas=(0.9, 0.999))\n",
    "\n",
    "    # Learning rate scheduler with warmup\n",
    "    warmup_epochs = 10\n",
    "    scheduler = WarmupCosineScheduler(\n",
    "        optimizer,\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        max_epochs=num_epochs,\n",
    "        warmup_start_lr=1e-6,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Gradient accumulation steps (for larger effective batch size)\n",
    "    accumulation_steps = 2\n",
    "\n",
    "    # Training and validation history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    # Best model tracking\n",
    "    best_val_acc = 0.0\n",
    "    patience = 5  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Checkpoint directory on Google Drive\n",
    "    checkpoint_dir = f\"/content/drive/MyDrive/vit_pretrained_cifar100/checkpoints/{dataset_name}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Update learning rate\n",
    "        current_lr = scheduler.step(epoch)\n",
    "        history['lr'].append(current_lr[0])  # Log learning rate\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Apply MixUp augmentation with 80% probability\n",
    "            use_mixup = np.random.random() < 0.8\n",
    "            if use_mixup:\n",
    "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=0.8)\n",
    "                inputs, targets_a, targets_b = inputs.to(device), targets_a.to(device), targets_b.to(device)\n",
    "\n",
    "            # Zero gradients only when accumulation steps completed\n",
    "            if (batch_idx % accumulation_steps == 0):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(inputs)\n",
    "                if use_mixup:\n",
    "                    loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                # Scale loss by accumulation steps for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Step optimizer only at the end of accumulation steps\n",
    "            if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "                # Unscale before gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Step with gradient scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            # Track statistics (adjust for mixup)\n",
    "            if use_mixup:\n",
    "                # For MixUp, we need to compute acc differently\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "                _, predicted = outputs.max(1)\n",
    "\n",
    "                # Approximate correct predictions with dominant label\n",
    "                train_total += targets.size(0)\n",
    "                if lam > 0.5:\n",
    "                    train_correct += predicted.eq(targets_a).sum().item()\n",
    "                else:\n",
    "                    train_correct += predicted.eq(targets_b).sum().item()\n",
    "            else:\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += targets.size(0)\n",
    "                train_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': train_loss / (batch_idx + 1),\n",
    "                'acc': 100. * train_correct / train_total,\n",
    "                'lr': current_lr[0]\n",
    "            })\n",
    "\n",
    "            # Free up GPU memory\n",
    "            del inputs, outputs, loss\n",
    "            if use_mixup:\n",
    "                del targets_a, targets_b\n",
    "            else:\n",
    "                del targets\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Calculate average training metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):  # Use mixed precision for validation as well\n",
    "                pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "                for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    # Track statistics\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += targets.size(0)\n",
    "                    val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': val_loss / (batch_idx + 1),\n",
    "                        'acc': 100. * val_correct / val_total\n",
    "                    })\n",
    "\n",
    "                    # Free up GPU memory\n",
    "                    del inputs, targets, outputs, loss\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        # Calculate average validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% - \"\n",
    "              f\"LR: {current_lr[0]:.6f} - \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Plot training progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:  # Every 5 epochs\n",
    "            plot_training_progress(history, dataset_name)\n",
    "\n",
    "        # Save best model and check early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "            # Save the best model\n",
    "            save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "                      checkpoint_dir=checkpoint_dir, is_best=True)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation accuracy did not improve. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "                      checkpoint_dir=checkpoint_dir, is_best=False,\n",
    "                      checkpoint_name=f\"checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "    # Save final model\n",
    "    save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "              checkpoint_dir=checkpoint_dir, is_best=False)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Functions for saving, evaluating, and loading models\n",
    "def save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "              checkpoint_dir=None, is_best=False, checkpoint_name=None):\n",
    "    \"\"\"\n",
    "    Save model checkpoint\n",
    "    \"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = f\"/content/drive/MyDrive/vit_pretrained_cifar100/checkpoints/{dataset_name}\"\n",
    "\n",
    "    if checkpoint_name:\n",
    "        model_type = checkpoint_name\n",
    "    else:\n",
    "        model_type = 'best' if is_best else 'final'\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"vit_{model_type}_checkpoint.pth\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history,\n",
    "        'class_info': class_info\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Saved {model_type} model checkpoint to {checkpoint_path}\")\n",
    "\n",
    "def evaluate_model(model, test_loader, class_info):\n",
    "    \"\"\"\n",
    "    Evaluate model on the full test set with consistent class mapping\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # Class-wise accuracy\n",
    "    class_correct = {}\n",
    "    class_total = {}\n",
    "\n",
    "    # Initialize counters for each class\n",
    "    for cls in range(class_info['n_classes']):\n",
    "        class_correct[cls] = 0\n",
    "        class_total[cls] = 0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create proper reverse mapping using class_mapping from class_info\n",
    "    if 'class_mapping' in class_info:\n",
    "        # Use the stored mapping if available\n",
    "        inverse_mapping = {i: cls for cls, i in class_info['class_mapping'].items()}\n",
    "    else:\n",
    "        # Fallback to create mapping from pretrain_classes\n",
    "        inverse_mapping = {i: cls for i, cls in enumerate(class_info['pretrain_classes'])}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Use mixed precision for evaluation - fixed deprecated API\n",
    "        from torch.amp.autocast_mode import autocast\n",
    "        with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            pbar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "            for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Track overall statistics - fix the class mapping issue\n",
    "                _, predicted_indices = outputs.max(1)\n",
    "\n",
    "                # SAFER APPROACH: Only count samples where the original class is in pretrain_classes\n",
    "                mask = torch.tensor([t.item() in class_info['pretrain_classes'] for t in targets.cpu()],\n",
    "                                    device=device)\n",
    "\n",
    "                # Filtered statistics\n",
    "                test_total += mask.sum().item()\n",
    "\n",
    "                correct_predictions = torch.zeros_like(targets, dtype=torch.bool)\n",
    "                for i, (pred_idx, target) in enumerate(zip(predicted_indices, targets)):\n",
    "                    if pred_idx.item() in inverse_mapping and target.item() in class_info['pretrain_classes']:\n",
    "                        orig_class = inverse_mapping[pred_idx.item()]\n",
    "                        if orig_class == target.item():\n",
    "                            correct_predictions[i] = True\n",
    "\n",
    "                test_correct += (correct_predictions & mask).sum().item()\n",
    "\n",
    "                # Track class-wise statistics - only for classes in pretrain_classes\n",
    "                for cls in class_info['pretrain_classes']:\n",
    "                    cls_idx = (targets == cls)\n",
    "                    class_total[cls] += cls_idx.sum().item()\n",
    "                    class_correct[cls] += (correct_predictions & cls_idx).sum().item()\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'acc': 100. * test_correct / test_total if test_total > 0 else 0\n",
    "                })\n",
    "\n",
    "                # Free up GPU memory\n",
    "                del inputs, targets, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate average test metrics\n",
    "    test_acc = 100. * test_correct / test_total if test_total > 0 else 0\n",
    "\n",
    "    print(f\"Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # Calculate accuracy for pretrain and continual classes\n",
    "    pretrain_correct = sum(class_correct[cls] for cls in class_info['pretrain_classes'])\n",
    "    pretrain_total = sum(class_total[cls] for cls in class_info['pretrain_classes'])\n",
    "    pretrain_acc = 100. * pretrain_correct / pretrain_total if pretrain_total > 0 else 0\n",
    "\n",
    "    continual_correct = sum(class_correct.get(cls, 0) for cls in class_info['continual_classes'])\n",
    "    continual_total = sum(class_total.get(cls, 0) for cls in class_info['continual_classes'])\n",
    "    continual_acc = 100. * continual_correct / continual_total if continual_total > 0 else 0\n",
    "\n",
    "    print(f\"Pretrain Classes Acc: {pretrain_acc:.2f}%\")\n",
    "    print(f\"Continual Classes Acc: {continual_acc:.2f}%\")\n",
    "\n",
    "    return test_acc, {\n",
    "        'pretrain_acc': pretrain_acc,\n",
    "        'continual_acc': continual_acc,\n",
    "        'class_acc': {cls: 100. * class_correct.get(cls, 0) / class_total.get(cls, 1)\n",
    "                     for cls in range(class_info['n_classes'])}\n",
    "    }\n",
    "\n",
    "def load_pretrained_model(dataset_name, model_type='best', checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Load a pretrained ViT model\n",
    "    \"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = f\"/content/drive/MyDrive/vit_pretrained_cifar100/checkpoints/{dataset_name}\"\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"vit_{model_type}_checkpoint.pth\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    class_info = checkpoint['class_info']\n",
    "    model = create_vit_model_from_scratch(num_classes=len(class_info['pretrain_classes']))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Loaded {model_type} {dataset_name} ViT model from {checkpoint_path}\")\n",
    "    print(f\"Model was trained for {checkpoint['epoch'] + 1} epochs\")\n",
    "\n",
    "    return model, class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 10: Visualization functions\n",
    "def plot_training_progress(history, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot training progress during training\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{dataset_name} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title(f'{dataset_name} Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot learning rate\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['lr'])\n",
    "    plt.title(f'{dataset_name} Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot complete training history after training\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{dataset_name} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title(f'{dataset_name} Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot learning rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['lr'])\n",
    "    plt.title(f'{dataset_name} Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Save the figure to Google Drive\n",
    "    save_path = f\"/content/drive/MyDrive/vit_pretrained_cifar100/{dataset_name}_training_history.png\"\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Saved training history plot to {save_path}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main training function\n",
    "def train_cifar100():\n",
    "    \"\"\"\n",
    "    Train ViT on CIFAR-100 from scratch\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    print(\"=== Pretraining ViT on CIFAR-100 from scratch ===\")\n",
    "    cifar_train_loader, cifar_val_loader, cifar_test_loader, cifar_class_info = prepare_dataset('cifar100')\n",
    "\n",
    "    cifar_model = create_vit_model_from_scratch(num_classes=len(cifar_class_info['pretrain_classes']))\n",
    "    cifar_model, cifar_history = train_model_from_scratch(\n",
    "        cifar_model, cifar_train_loader, cifar_val_loader, cifar_class_info, 'cifar100'\n",
    "    )\n",
    "\n",
    "    # print(\"\\n=== Evaluating CIFAR-100 ViT on Full Test Set ===\")\n",
    "    # evaluate_model(cifar_model, cifar_test_loader, cifar_class_info)\n",
    "\n",
    "    # Plot final training history\n",
    "    plot_training_history(cifar_history, 'cifar100')\n",
    "\n",
    "    print(\"\\nCIFAR-100 pretraining complete! Model saved to Google Drive.\")\n",
    "    return cifar_model, cifar_history, cifar_class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to run the CIFAR-100 training\n",
    "cifar_model, cifar_history, cifar_class_info = train_cifar100()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omccood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
