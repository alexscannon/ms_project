{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, restart your Colab runtime completely:\n",
    "# Runtime â†’ Restart runtime\n",
    "\n",
    "# Then add this at the very beginning of your notebook:\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set memory optimization BEFORE any model creation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear any existing variables\n",
    "for var in list(globals().keys()):\n",
    "    if var not in ['__builtins__', '__name__', '__doc__', '__loader__']:\n",
    "        del globals()[var]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm tqdm matplotlib\n",
    "\n",
    "# Cell 1: Mount Google Drive for saving models and checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_RUN = 5 # Update for every new configuration\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Dataset parameters\n",
    "IND_CLASS_RATIO = 0.80\n",
    "PRETRAIN_EXAMPLE_RATIO = 0.75\n",
    "VAL_RATIO = 0.20\n",
    "CIFAR100_NUM_CLASSES = 100\n",
    "BATCH_SIZE = 16\n",
    "NUM_OF_WORKERS = 1\n",
    "\n",
    "# Model parameters\n",
    "MODEL = 'vit_small_patch16_224'\n",
    "DROPOUT_PATH_RATE = 0.10\n",
    "DROPOUT_RATE = 0.10\n",
    "PATCH_SIZE = 2\n",
    "\n",
    "# Optimizer parameters\n",
    "WARMUP_LR_START = 1e-6\n",
    "ETA_MIN = 1e-6\n",
    "LR_WARMUP_EPOCHS = 20\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_EPOCHS = 200\n",
    "OPTIMIZER_LR = 5e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "BETAS = (0.9, 0.999)\n",
    "ACCUMULATION_STEPS = 16\n",
    "PATIENCE_EPOCHS = 10\n",
    "LABEL_SMOOTHING = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for checkpoints\n",
    "CHECKPOINT_DIR_PATH = f\"/content/drive/MyDrive/vit_pretrained_cifar100/checkpoints/cifar100_{EXPERIMENT_RUN}\"\n",
    "!mkdir -p dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries and set up device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # Use notebook version for better Colab progress bars\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# For ViT model\n",
    "import timm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) # set numpy seed\n",
    "    torch.manual_seed(seed) # set torch (CPU) seed\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed) # set torch (GPU) seed\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "        torch.backends.cudnn.deterministic = True # set cudnn to deterministic mode\n",
    "        torch.backends.cudnn.benchmark = False  # Disable benchmarking for reproducibility\n",
    "    # Set seeds for data loading operations\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # Document the environment for future reproducibility\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\")\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Worker initialization function for DataLoaders\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Set device - Colab typically provides a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create a data wrapper class to remap the class lables\n",
    "class ClassRemappingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, class_mapping):\n",
    "        self.dataset = dataset\n",
    "        self.class_mapping = class_mapping\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[index]\n",
    "        # Map the original class index to the new consecutive index\n",
    "        new_target = self.class_mapping[target]\n",
    "        return img, new_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset preparation function - updated to track left-out examples\n",
    "def prepare_dataset(data_dir='/content/drive/MyDrive/data'):\n",
    "    \"\"\"\n",
    "    Prepare dataset for pretraining according to requirements:\n",
    "    - 80% of classes for pretraining\n",
    "    - 75% of each pretraining class examples\n",
    "    - 20% of classes reserved for continual learning\n",
    "    Now also tracks the 25% left-out examples from pretraining classes for continual learning\n",
    "    \"\"\"\n",
    "\n",
    "    # Define transforms with stronger augmentation for training from scratch\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(224),  # Resize to ViT input size\n",
    "        transforms.TrivialAugmentWide(),  # Better than RandAugment\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5071, 0.4867, 0.4408),\n",
    "            std=(0.2675, 0.2565, 0.2761)\n",
    "        ),\n",
    "        transforms.RandomErasing(p=0.15)\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(224),  # Resize to ViT input size of 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5071, 0.4867, 0.4408),\n",
    "            std=(0.2675, 0.2565, 0.2761)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-100 dataset\n",
    "    train_dataset = CIFAR100(root=data_dir, train=True, download=True, transform=train_transform)\n",
    "    test_dataset = CIFAR100(root=data_dir, train=False, download=True, transform=test_transform)\n",
    "\n",
    "    # Number of classes\n",
    "    num_of_classes = CIFAR100_NUM_CLASSES\n",
    "    num_of_pretrain_classes = int(IND_CLASS_RATIO * num_of_classes)\n",
    "\n",
    "    all_class_indicies = list(range(num_of_classes))\n",
    "    random.shuffle(all_class_indicies) # Shuffle the class indicies before splitting into pretraining and OOD classes\n",
    "\n",
    "    pretrain_classes = all_class_indicies[:num_of_pretrain_classes]\n",
    "    ood_classes = all_class_indicies[num_of_pretrain_classes:]\n",
    "\n",
    "    ind_class_mapping = {class_idx: i for i, class_idx in enumerate(pretrain_classes)}\n",
    "    # ex. pretrain_class_mapping = {23: 0, 11: 1, 93: 2, ...}\n",
    "\n",
    "    print(f\"Selected {len(pretrain_classes)} classes for pretraining\")\n",
    "    print(f\"Reserved {len(ood_classes)} classes for OOD detection\")\n",
    "\n",
    "    train_indices = [i for i, (input_tensor, label) in enumerate(train_dataset) if label in pretrain_classes]\n",
    "\n",
    "    class_indices = {} # Maps indices of examples to their class labels\n",
    "    for idx in train_indices:\n",
    "        input_tensor, label = train_dataset[idx]\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    pretrained_ind_indices, pretrained_left_out_indices = [], []\n",
    "    for label, example_indices in class_indices.items():\n",
    "        num_of_examples = len(example_indices)\n",
    "        num_of_pretrain_examples = int(PRETRAIN_EXAMPLE_RATIO * num_of_examples)\n",
    "        pretrained_ind_indices.extend(example_indices[:num_of_pretrain_examples])\n",
    "        pretrained_left_out_indices.extend(example_indices[num_of_pretrain_examples:])  # Store the 25% left-out examples\n",
    "\n",
    "    # Create subset datasets\n",
    "    print(\"Creating subset datasets...\")\n",
    "    pretrain_subset = Subset(train_dataset, pretrained_ind_indices)\n",
    "    pretrain_dataset = ClassRemappingDataset(pretrain_subset, ind_class_mapping)\n",
    "\n",
    "    # Create train-val split\n",
    "    n_pretrain = len(pretrain_dataset)\n",
    "    n_val = int(VAL_RATIO * n_pretrain)\n",
    "    n_train = n_pretrain - n_val\n",
    "\n",
    "    # Use generator for reproducible split\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    pretrain_train_dataset, pretrain_val_dataset = random_split(\n",
    "        pretrain_dataset, [n_train, n_val], generator=generator\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    batch_size = BATCH_SIZE\n",
    "\n",
    "    pretrain_loader = DataLoader(\n",
    "        pretrain_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_OF_WORKERS,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        pretrain_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_OF_WORKERS,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    full_test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_OF_WORKERS,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    print(f\"Pretraining on {len(pretrain_train_dataset)} samples\")\n",
    "    print(f\"Validation on {len(pretrain_val_dataset)} samples\")\n",
    "    print(f\"Full test set has {len(test_dataset)} samples\")\n",
    "    print(f\"Left out {len(pretrained_left_out_indices)} examples from pretraining classes for continual learning\")\n",
    "\n",
    "    # Store class information\n",
    "    class_info = {\n",
    "        'num_of_classes': num_of_classes, # Old name for this property \"n_classes\"\n",
    "        'pretrain_classes': pretrain_classes,\n",
    "        'left_out_classes': ood_classes, # Old name for this property \"continual_classes\"\n",
    "        'left_out_ind_indices': pretrained_left_out_indices, # Old name for this property \"left_out_indices\"\n",
    "        'pretrained_ind_indices': pretrained_ind_indices,\n",
    "        'pretrain_class_mapping': ind_class_mapping # Old name for this property \"class_mapping\"\n",
    "    }\n",
    "\n",
    "    return pretrain_loader, val_loader, full_test_loader, class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model creation and learning rate scheduler\n",
    "def create_vit_model_from_scratch(num_classes):\n",
    "    \"\"\"\n",
    "    Create a ViT model from scratch (without pre-trained weights)\n",
    "    \"\"\"\n",
    "    # Create ViT model with random initialization (pretrained=False)\n",
    "    model = timm.create_model(\n",
    "        MODEL,\n",
    "        pretrained=False,\n",
    "        num_classes=num_classes,\n",
    "        drop_path_rate=DROPOUT_PATH_RATE,\n",
    "        drop_rate=DROPOUT_RATE,\n",
    "        patch_size=PATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Better initialization for Transformers\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    model.apply(_init_weights)\n",
    "    return model\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "class WarmupCosineScheduler():\n",
    "    def __init__(self, optimizer, warmup_epochs, max_epochs):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = WARMUP_LR_START\n",
    "        self.eta_min = ETA_MIN\n",
    "\n",
    "        # Get base lr\n",
    "        self.base_lr = []\n",
    "        for group in optimizer.param_groups:\n",
    "            self.base_lr.append(group['lr'])\n",
    "\n",
    "    def step(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr_mult = epoch / self.warmup_epochs\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                group['lr'] = self.warmup_start_lr + lr_mult * (self.base_lr[i] - self.warmup_start_lr)\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            for i, group in enumerate(self.optimizer.param_groups):\n",
    "                progress = (epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)\n",
    "                cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "                group['lr'] = self.eta_min + cosine_decay * (self.base_lr[i] - self.eta_min)\n",
    "\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MixUp and CutMix data augmentation\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_from_scratch(model, train_loader, val_loader, class_info, dataset_name,\n",
    "                           num_epochs=1):\n",
    "    \"\"\"\n",
    "    Train the ViT model from scratch with proper hyperparameters and mixed precision\n",
    "    \"\"\"\n",
    "    # Import for mixed precision training\n",
    "    from torch.amp.autocast_mode import autocast\n",
    "    from torch.amp import GradScaler\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=OPTIMIZER_LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        betas=BETAS\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler with warmup\n",
    "    scheduler = WarmupCosineScheduler(\n",
    "        optimizer,\n",
    "        warmup_epochs=LR_WARMUP_EPOCHS,\n",
    "        max_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler('cuda', enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # Gradient accumulation steps (for larger effective batch size)\n",
    "    accumulation_steps = ACCUMULATION_STEPS\n",
    "\n",
    "    # Training and validation history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    # Best model tracking\n",
    "    best_val_acc = 0.0\n",
    "    patience = PATIENCE_EPOCHS  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Checkpoint directory on Google Drive\n",
    "    checkpoint_dir = CHECKPOINT_DIR_PATH\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"<======= Starting training for {num_epochs} epochs... =======>\")\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Update learning rate\n",
    "        current_lr = scheduler.step(epoch)\n",
    "        history['lr'].append(current_lr[0])  # Log learning rate\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "            # Apply MixUp augmentation with 90% probability\n",
    "            use_mixup = np.random.random() < 0.9\n",
    "            if use_mixup:\n",
    "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=1.0)\n",
    "                # âœ… FIX: Don't move inputs to device again - already there\n",
    "                targets_a, targets_b = targets_a.to(device, non_blocking=True), targets_b.to(device, non_blocking=True)\n",
    "\n",
    "            # Zero gradients only when accumulation steps completed\n",
    "            if (batch_idx % accumulation_steps == 0):\n",
    "                optimizer.zero_grad(set_to_none=True)  # âœ… FIX: Use set_to_none=True\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(inputs)\n",
    "                if use_mixup:\n",
    "                    loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                # Scale loss by accumulation steps for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            # Backward pass (w/ gradient scaling)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Step optimizer only at the end of accumulation steps\n",
    "            if ((batch_idx + 1) % accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "                # Unscale before gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Step with gradient scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # âœ… FIX: Zero gradients after step\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Track statistics (adjust for mixup)\n",
    "            with torch.no_grad():  # âœ… FIX: Wrap statistics in no_grad\n",
    "                if use_mixup:\n",
    "                    train_loss += loss.item() * accumulation_steps\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    train_total += targets.size(0)\n",
    "                    if lam > 0.5:\n",
    "                        train_correct += predicted.eq(targets_a).sum().item()\n",
    "                    else:\n",
    "                        train_correct += predicted.eq(targets_b).sum().item()\n",
    "                else:\n",
    "                    train_loss += loss.item() * accumulation_steps\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    train_total += targets.size(0)\n",
    "                    train_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': train_loss / (batch_idx + 1),\n",
    "                'acc': 100. * train_correct / train_total,\n",
    "                'lr': current_lr[0]\n",
    "            })\n",
    "\n",
    "            # âœ… FIX: Minimal memory cleanup - NO torch.cuda.empty_cache()!\n",
    "            # Only clear memory occasionally, not every batch\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0:  # Every 100 batches\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Calculate average training metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        # âœ… OPTIMIZED VALIDATION LOOP\n",
    "        with torch.no_grad():\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "                for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "                    inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    # Track statistics\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += targets.size(0)\n",
    "                    val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': val_loss / (batch_idx + 1),\n",
    "                        'acc': 100. * val_correct / val_total\n",
    "                    })\n",
    "\n",
    "                    # âœ… FIX: Only clear memory occasionally\n",
    "                    if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "        # Calculate average validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% - \"\n",
    "              f\"LR: {current_lr[0]:.6f} - \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Plot training progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:  # Every 5 epochs\n",
    "            plot_training_progress(history, dataset_name)\n",
    "\n",
    "        # Save best model and check early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "            # Save the best model\n",
    "            save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "                      checkpoint_dir=checkpoint_dir, is_best=True)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation accuracy did not improve. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "                      checkpoint_dir=checkpoint_dir, is_best=False,\n",
    "                      checkpoint_name=f\"checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "    # Save final model\n",
    "    save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "              checkpoint_dir=checkpoint_dir, is_best=False)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Functions for saving, evaluating, and loading models\n",
    "def save_model(model, optimizer, epoch, history, class_info, dataset_name,\n",
    "              checkpoint_dir=None, is_best=False, checkpoint_name=None):\n",
    "    \"\"\"\n",
    "    Save model checkpoint\n",
    "    \"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = CHECKPOINT_DIR_PATH\n",
    "\n",
    "    if checkpoint_name:\n",
    "        model_type = checkpoint_name\n",
    "    else:\n",
    "        model_type = 'best' if is_best else 'final'\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"vit_{model_type}_checkpoint.pth\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(), # Model weights\n",
    "        'optimizer_state_dict': optimizer.state_dict(), # Optimizer state\n",
    "        'history': history, # Training history\n",
    "        'class_info': class_info # Class information\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Saved {model_type} model checkpoint to {checkpoint_path}\")\n",
    "\n",
    "# Currently not used but could be used to evaluate the model on the full test set\n",
    "# def evaluate_model(model, test_loader, class_info):\n",
    "#     \"\"\"\n",
    "#     Evaluate model on the full test set with consistent class mapping\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "\n",
    "#     # Class-wise accuracy\n",
    "#     class_correct = {}\n",
    "#     class_total = {}\n",
    "\n",
    "#     # Initialize counters for each class\n",
    "#     for cls in range(class_info['n_classes']):\n",
    "#         class_correct[cls] = 0\n",
    "#         class_total[cls] = 0\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Create proper reverse mapping using class_mapping from class_info\n",
    "#     if 'class_mapping' in class_info:\n",
    "#         # Use the stored mapping if available\n",
    "#         inverse_mapping = {i: cls for cls, i in class_info['class_mapping'].items()}\n",
    "#     else:\n",
    "#         # Fallback to create mapping from pretrain_classes\n",
    "#         inverse_mapping = {i: cls for i, cls in enumerate(class_info['pretrain_classes'])}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # Use mixed precision for evaluation - fixed deprecated API\n",
    "#         from torch.amp.autocast_mode import autocast\n",
    "#         with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "#             pbar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "#             for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "#                 inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "\n",
    "#                 # Track overall statistics - fix the class mapping issue\n",
    "#                 _, predicted_indices = outputs.max(1)\n",
    "\n",
    "#                 # SAFER APPROACH: Only count samples where the original class is in pretrain_classes\n",
    "#                 mask = torch.tensor([t.item() in class_info['pretrain_classes'] for t in targets.cpu()],\n",
    "#                                     device=device)\n",
    "\n",
    "#                 # Filtered statistics\n",
    "#                 test_total += mask.sum().item()\n",
    "\n",
    "#                 correct_predictions = torch.zeros_like(targets, dtype=torch.bool)\n",
    "#                 for i, (pred_idx, target) in enumerate(zip(predicted_indices, targets)):\n",
    "#                     if pred_idx.item() in inverse_mapping and target.item() in class_info['pretrain_classes']:\n",
    "#                         orig_class = inverse_mapping[pred_idx.item()]\n",
    "#                         if orig_class == target.item():\n",
    "#                             correct_predictions[i] = True\n",
    "\n",
    "#                 test_correct += (correct_predictions & mask).sum().item()\n",
    "\n",
    "#                 # Track class-wise statistics - only for classes in pretrain_classes\n",
    "#                 for cls in class_info['pretrain_classes']:\n",
    "#                     cls_idx = (targets == cls)\n",
    "#                     class_total[cls] += cls_idx.sum().item()\n",
    "#                     class_correct[cls] += (correct_predictions & cls_idx).sum().item()\n",
    "\n",
    "#                 # Update progress bar\n",
    "#                 pbar.set_postfix({\n",
    "#                     'acc': 100. * test_correct / test_total if test_total > 0 else 0\n",
    "#                 })\n",
    "\n",
    "#                 # Free up GPU memory\n",
    "#                 del inputs, targets, outputs\n",
    "#                 torch.cuda.empty_cache()\n",
    "\n",
    "#     # Calculate average test metrics\n",
    "#     test_acc = 100. * test_correct / test_total if test_total > 0 else 0\n",
    "\n",
    "#     print(f\"Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "#     # Calculate accuracy for pretrain and continual classes\n",
    "#     pretrain_correct = sum(class_correct[cls] for cls in class_info['pretrain_classes'])\n",
    "#     pretrain_total = sum(class_total[cls] for cls in class_info['pretrain_classes'])\n",
    "#     pretrain_acc = 100. * pretrain_correct / pretrain_total if pretrain_total > 0 else 0\n",
    "\n",
    "#     continual_correct = sum(class_correct.get(cls, 0) for cls in class_info['continual_classes'])\n",
    "#     continual_total = sum(class_total.get(cls, 0) for cls in class_info['continual_classes'])\n",
    "#     continual_acc = 100. * continual_correct / continual_total if continual_total > 0 else 0\n",
    "\n",
    "#     print(f\"Pretrain Classes Acc: {pretrain_acc:.2f}%\")\n",
    "#     print(f\"Continual Classes Acc: {continual_acc:.2f}%\")\n",
    "\n",
    "#     return test_acc, {\n",
    "#         'pretrain_acc': pretrain_acc,\n",
    "#         'continual_acc': continual_acc,\n",
    "#         'class_acc': {cls: 100. * class_correct.get(cls, 0) / class_total.get(cls, 1)\n",
    "#                      for cls in range(class_info['n_classes'])}\n",
    "#     }\n",
    "\n",
    "def load_pretrained_model(dataset_name, model_type='best', checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Load a pretrained ViT model\n",
    "    \"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = CHECKPOINT_DIR_PATH\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"vit_{model_type}_checkpoint.pth\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    class_info = checkpoint['class_info']\n",
    "    model = create_vit_model_from_scratch(num_classes=len(class_info['pretrain_classes']))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"Loaded {model_type} {dataset_name} ViT model from {checkpoint_path}\")\n",
    "    print(f\"Model was trained for {checkpoint['epoch'] + 1} epochs\")\n",
    "\n",
    "    return model, class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 10: Visualization functions\n",
    "def plot_training_progress(history, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot training progress during training\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{dataset_name} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title(f'{dataset_name} Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot learning rate\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['lr'])\n",
    "    plt.title(f'{dataset_name} Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot complete training history after training\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{dataset_name} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title(f'{dataset_name} Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot learning rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['lr'])\n",
    "    plt.title(f'{dataset_name} Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Save the figure to Google Drive\n",
    "    save_path = f\"{CHECKPOINT_DIR_PATH}_training_history.png\"\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Saved training history plot to {save_path}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main training function\n",
    "def train_cifar100():\n",
    "    \"\"\"\n",
    "    Train ViT on CIFAR-100 from scratch\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    print(\"=== Pretraining ViT on CIFAR-100 from scratch ===\")\n",
    "    cifar_train_loader, cifar_val_loader, cifar_test_loader, cifar_class_info = prepare_dataset()\n",
    "\n",
    "    cifar_model = create_vit_model_from_scratch(num_classes=len(cifar_class_info['pretrain_classes']))\n",
    "    cifar_model, cifar_history = train_model_from_scratch(\n",
    "        cifar_model, cifar_train_loader, cifar_val_loader, cifar_class_info, 'cifar100', TRAINING_EPOCHS\n",
    "    )\n",
    "\n",
    "    # print(\"\\n=== Evaluating CIFAR-100 ViT on Full Test Set ===\")\n",
    "    # evaluate_model(cifar_model, cifar_test_loader, cifar_class_info)\n",
    "\n",
    "    # Plot final training history\n",
    "    plot_training_history(cifar_history, 'cifar100')\n",
    "\n",
    "    print(\"\\nCIFAR-100 pretraining complete! Model saved to Google Drive.\")\n",
    "    return cifar_model, cifar_history, cifar_class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated, \"\n",
    "              f\"{torch.cuda.memory_reserved() / 1e9:.2f} GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to run the CIFAR-100 training\n",
    "cifar_model, cifar_history, cifar_class_info = train_cifar100()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omccood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
