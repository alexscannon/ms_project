{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import shutil\n",
    "import urllib.request\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm  # Use notebook tqdm for Colab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.amp.autocast_mode import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "TINY_IMAGENET_URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip' # Or use .tar.gz if preferred\n",
    "DRIVE_MOUNT_POINT = '/content/drive/MyDrive/' # Optional: Google Drive mount point\n",
    "DATA_DIR = DRIVE_MOUNT_POINT + 'data/tiny-imagenet-200-1'\n",
    "SAVE_DIR = DRIVE_MOUNT_POINT + 'vit_pretrained_tinyimagenet'\n",
    "OUTPUT_DIR = 'output_data_splits' # Directory to save split info and model\n",
    "SAVE_TO_DRIVE = True # Set to True to save outputs to Google Drive/My Drive/Colab Outputs\n",
    "\n",
    "# Splitting percentages\n",
    "ID_CLASS_RATIO = 0.80\n",
    "EXAMPLE_PRETRAIN_RATIO = 0.75\n",
    "# Model and Training Hyperparameters\n",
    "MODEL_NAME = 'vit_tiny_patch16_224'\n",
    "IMAGE_SIZE = 224\n",
    "MEAN=[0.485, 0.456, 0.406]\n",
    "STD=[0.229, 0.224, 0.225]\n",
    "\n",
    "BATCH_SIZE = 128 # Adjust based on Colab GPU memory (T4, V100, etc.)\n",
    "LEARNING_RATE = 1e-4 # Peak learning rate after warmup\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_EPOCHS = 5 # <<< Number of epochs for linear warmup <<< ADD THIS\n",
    "NUM_EPOCHS = 100 # Adjust as needed for convergence\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# <<< ADDED: Early Stopping Parameters >>>\n",
    "PATIENCE = 10 # How many epochs to wait for improvement before stopping\n",
    "EARLY_STOPPING_METRIC = 'val_acc' # Metric to monitor ('val_acc' or 'val_loss')\n",
    "MIN_DELTA = 0.001 # Minimum change to qualify as an improvement (for val_acc, use positive; for val_loss, use negative)\n",
    "\n",
    "# <<< ADDED: Label Smoothing Parameter >>>\n",
    "LABEL_SMOOTHING = 0.1 # Factor for label smoothing (0.0 means no smoothing)\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a directory for our dataset\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(f\"Data will be saved in: {DATA_DIR}\")\n",
    "\n",
    "\n",
    "# URL for the Tiny-ImageNet dataset\n",
    "zip_path = os.path.join(DATA_DIR, 'tiny-imagenet-200.zip')\n",
    "\n",
    "# Download the dataset if it doesn't exist\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading Tiny-ImageNet dataset...\")\n",
    "\n",
    "    # Create a progress bar for download\n",
    "    def report_progress(block_num, block_size, total_size):\n",
    "        progress = float(block_num * block_size) / float(total_size) * 100.0\n",
    "        print(f\"\\rDownloading: {progress:.2f}%\", end=\"\")\n",
    "\n",
    "    # Download with progress reporting\n",
    "    urllib.request.urlretrieve(TINY_IMAGENET_URL, zip_path, reporthook=report_progress)\n",
    "    print(\"\\nDownload complete!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded.\")\n",
    "\n",
    "# Extract the dataset if not already extracted\n",
    "extract_dir = os.path.join(DATA_DIR, 'tiny-imagenet-200')\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")\n",
    "\n",
    "# Basic validation to check the dataset structure\n",
    "train_dir = os.path.join(extract_dir, 'train')\n",
    "val_dir = os.path.join(extract_dir, 'val')\n",
    "\n",
    "if os.path.exists(train_dir) and os.path.exists(val_dir):\n",
    "    # Count the number of classes in training set\n",
    "    train_classes = os.listdir(train_dir)\n",
    "    print(f\"Number of classes in training set: {len(train_classes)}\")\n",
    "\n",
    "    # Check a few example classes\n",
    "    print(f\"Example classes: {train_classes[:5]}\")\n",
    "\n",
    "    # Check the structure of one class\n",
    "    example_class = train_classes[0]\n",
    "    example_class_dir = os.path.join(train_dir, example_class)\n",
    "    example_images_dir = os.path.join(example_class_dir, 'images')\n",
    "    example_images = os.listdir(example_images_dir)\n",
    "    print(f\"Number of images in {example_class}: {len(example_images)}\")\n",
    "    print(f\"Example image paths: {example_images[:3]}\")\n",
    "\n",
    "    print(\"Dataset structure validation complete!\")\n",
    "else:\n",
    "    print(\"Dataset structure seems incorrect. Please check the extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Copy data from Drive to local Colab storage (if not already local) ---\n",
    "# Assumes your data is in Drive at this path:\n",
    "gdrive_data_dir = DATA_DIR # ADJUST THIS PATH if needed\n",
    "local_data_dir = '/content/tiny-imagenet-local' # Destination on Colab's fast local disk\n",
    "\n",
    "print(f\"Source data path (Google Drive): {gdrive_data_dir}\")\n",
    "print(f\"Local data destination: {local_data_dir}\")\n",
    "\n",
    "start_copy_time = time.time()\n",
    "if os.path.exists(gdrive_data_dir):\n",
    "    if not os.path.exists(local_data_dir):\n",
    "        print(\"Copying dataset from Google Drive to local Colab storage...\")\n",
    "        print(\"This might take 10 - 20 minutes...\")\n",
    "        try:\n",
    "            # Use shutil.copytree for simplicity, though rsync can be faster if available/needed\n",
    "            shutil.copytree(gdrive_data_dir, local_data_dir)\n",
    "            # OR using rsync (often faster, handles interruptions better, might need installation)\n",
    "            print(f\"Dataset copied successfully to {local_data_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR copying dataset: {e}\")\n",
    "            print(\"Proceeding might be extremely slow if using Drive directly.\")\n",
    "            # Decide how to handle error: exit, or try to use Drive path?\n",
    "            # For now, we'll assume the user wants to continue, but warn them.\n",
    "            # Set DATA_DIR to the Drive path as a fallback IF local copy failed.\n",
    "            # DATA_DIR = gdrive_data_dir # Fallback - uncomment if needed\n",
    "    else:\n",
    "        print(\"Dataset already exists in local Colab storage.\")\n",
    "    # *** IMPORTANT: Update DATA_DIR to use the local path ***\n",
    "    DATA_DIR = local_data_dir\n",
    "else:\n",
    "    print(f\"Warning: Google Drive data directory not found at {gdrive_data_dir}\")\n",
    "    print(\"Assuming data might already be local or script needs adjustment.\")\n",
    "    # If you previously downloaded directly to /content/tiny-imagenet-200, set that path\n",
    "    if os.path.exists('/content/tiny-imagenet-200'):\n",
    "         DATA_DIR = '/content/tiny-imagenet-200' # Example if downloaded directly\n",
    "         print(f\"Using existing directory: {DATA_DIR}\")\n",
    "    else:\n",
    "         # If neither Drive path nor default local path exists, raise error or set expected path\n",
    "         print(f\"ERROR: Cannot find data. Set DATA_DIR manually or check paths.\")\n",
    "         # DATA_DIR = '/content/tiny...' # Set your expected path here if needed\n",
    "         # exit() # Or exit if data is missing\n",
    "\n",
    "end_copy_time = time.time()\n",
    "print(f\"Data setup took {end_copy_time - start_copy_time:.2f} seconds.\")\n",
    "print(f\"Using DATA_DIR: {DATA_DIR}\") # Verify this path is used later\n",
    "\n",
    "# --- Now proceed with the rest of your script ---\n",
    "# Ensure Steps 2, 5, etc., use this updated DATA_DIR variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Class Information ---\n",
    "wnids_path = os.path.join(extract_dir, 'wnids.txt')\n",
    "words_path = os.path.join(extract_dir, 'words.txt')\n",
    "\n",
    "if not os.path.exists(wnids_path) or not os.path.exists(words_path):\n",
    "     print(f\"Error: Cannot find {wnids_path} or {words_path}. Dataset might be corrupted or incomplete.\")\n",
    "     # Exit or handle error appropriately\n",
    "     # For now, we'll stop if these critical files are missing after download/extract attempt.\n",
    "     exit()\n",
    "\n",
    "\n",
    "all_wnids = []\n",
    "with open(wnids_path, 'r') as f:\n",
    "    all_wnids = [line.strip() for line in f]\n",
    "num_total_classes = len(all_wnids)\n",
    "print(f\"Total classes found: {num_total_classes}\")\n",
    "\n",
    "wnid_to_name = {}\n",
    "with open(words_path, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            wnid_to_name[parts[0]] = parts[1]\n",
    "\n",
    "# --- 3. Create Mappings ---\n",
    "wnid_to_idx = {wnid: i for i, wnid in enumerate(all_wnids)}\n",
    "idx_to_wnid = {i: wnid for wnid, i in wnid_to_idx.items()}\n",
    "\n",
    "# --- 4. Class Split (ID vs OOD) ---\n",
    "num_id_classes = int(num_total_classes * ID_CLASS_RATIO)\n",
    "num_ood_classes = num_total_classes - num_id_classes\n",
    "\n",
    "print(f\"Splitting classes: {num_id_classes} ID classes, {num_ood_classes} OOD classes.\")\n",
    "\n",
    "shuffled_wnids = list(all_wnids)\n",
    "random.shuffle(shuffled_wnids) # Shuffle wnids randomly\n",
    "\n",
    "id_wnids = sorted(shuffled_wnids[:num_id_classes]) # Sort for consistency\n",
    "ood_wnids = sorted(shuffled_wnids[num_id_classes:]) # Sort for consistency\n",
    "\n",
    "print(f\"\\nSelected {len(id_wnids)} ID WNIDs (first 5): {id_wnids[:5]}\")\n",
    "print(f\"Selected {len(ood_wnids)} OOD WNIDs (first 5): {ood_wnids[:5]}\")\n",
    "\n",
    "# Map ID WNIDs to the new label space (0 to num_id_classes-1) for the classifier\n",
    "wnid_to_pretrain_label = {wnid: i for i, wnid in enumerate(id_wnids)}\n",
    "pretrain_label_to_wnid = {i: wnid for wnid, i in wnid_to_pretrain_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Prepare File Lists and Example Splits ---\n",
    "train_dir = os.path.join(extract_dir, 'train')\n",
    "\n",
    "pretrain_files = [] # List of (filepath, label_idx_0_to_159)\n",
    "# *** MODIFIED: Store (filepath, wnid) for reserved files ***\n",
    "id_reserved_files_with_wnid = []\n",
    "ood_class_files = [] # List of filepaths (examples from OOD classes)\n",
    "file_to_label_map = {} # Map filepath -> pretrain_label for the dataset\n",
    "\n",
    "print(\"\\nScanning training directory and splitting examples...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for wnid in tqdm(os.listdir(train_dir)):\n",
    "    class_dir = os.path.join(train_dir, wnid)\n",
    "    if os.path.isdir(class_dir):\n",
    "        images_dir = os.path.join(class_dir, 'images')\n",
    "        if not os.path.exists(images_dir):\n",
    "             print(f\"Warning: 'images' subdirectory not found in {class_dir}. Skipping this class.\")\n",
    "             continue\n",
    "\n",
    "        image_files = [os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        if wnid in id_wnids:\n",
    "            # In-Distribution class\n",
    "            num_examples = len(image_files)\n",
    "            num_pretrain = int(num_examples * EXAMPLE_PRETRAIN_RATIO)\n",
    "\n",
    "            pretrain_split = image_files[:num_pretrain]\n",
    "            reserved_split = image_files[num_pretrain:] # Paths for reserved files\n",
    "\n",
    "            # *** MODIFIED: Store (filepath, wnid) for reserved files ***\n",
    "            for fpath in reserved_split:\n",
    "                id_reserved_files_with_wnid.append((fpath, wnid)) # Store WNID\n",
    "\n",
    "            pretrain_label = wnid_to_pretrain_label[wnid]\n",
    "            for fpath in pretrain_split:\n",
    "                pretrain_files.append((fpath, pretrain_label))\n",
    "                file_to_label_map[fpath] = pretrain_label\n",
    "\n",
    "        elif wnid in ood_wnids:\n",
    "            # Out-of-Distribution class\n",
    "            ood_class_files.extend(image_files)\n",
    "        # else: # WNID not in ID or OOD lists (shouldn't happen with correct setup)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Finished splitting files in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Total pretraining examples: {len(pretrain_files)}\")\n",
    "print(f\"Total ID-reserved examples (for validation): {len(id_reserved_files_with_wnid)}\") # Modified print\n",
    "print(f\"Total OOD-class examples: {len(ood_class_files)}\")\n",
    "\n",
    "# --- Create Validation File List with Correct Labels ---\n",
    "validation_files = []\n",
    "print(\"\\nMapping reserved files to pretraining labels for validation set...\")\n",
    "for fpath, wnid in tqdm(id_reserved_files_with_wnid, desc=\"Mapping val files\"):\n",
    "    if wnid in wnid_to_pretrain_label: # Ensure wnid is an ID class (should always be true here)\n",
    "        pretrain_label = wnid_to_pretrain_label[wnid]\n",
    "        validation_files.append((fpath, pretrain_label))\n",
    "    else:\n",
    "        print(f\"Warning: WNID {wnid} from reserved files not found in pretrain label map. Skipping file {fpath}\")\n",
    "\n",
    "print(f\"Created validation file list with {len(validation_files)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Custom Dataset (No significant changes needed here) ---\n",
    "# The dataset remains simple, loading verified images one by one.\n",
    "class TinyImageNetPretrain(Dataset):\n",
    "    def __init__(self, file_label_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_label_list (list): List of tuples (verified_filepath, label_index).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.file_label_list = file_label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_label_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # We assume files exist and are readable due to pre-verification\n",
    "        img_path, label = self.file_label_list[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            # Should be rare now, but good to have a fallback\n",
    "            print(f\"ERROR in __getitem__ for {img_path}: {e}. Returning None.\")\n",
    "             # Returning None requires a collate_fn in DataLoader to handle it,\n",
    "             # or risking errors later. For simplicity, we'll rely on pre-verification.\n",
    "             # If errors still occur here, investigate the specific files.\n",
    "             # Re-raising might be better for debugging:\n",
    "            raise RuntimeError(f\"Failed to load image {img_path} during training\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Data Transformations (No changes needed) ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.6, 1.0), ratio=(0.75, 1.33), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "# --- 8. Create Datasets and DataLoaders (Optimized) ---\n",
    "pretrain_dataset = TinyImageNetPretrain(\n",
    "    file_label_list=pretrain_files, # Use the verified list\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# DataLoader Optimizations:\n",
    "# - num_workers: Increased (e.g., 4). Monitor Colab CPU/RAM. If issues occur, reduce to 2.\n",
    "# - persistent_workers: Reduces overhead between epochs (requires PyTorch >= 1.7, usually true on Colab)\n",
    "# - prefetch_factor: Controls batch preloading per worker. Default (2) is often good.\n",
    "# - pin_memory: Kept as True for faster CPU->GPU transfers.\n",
    "pretrain_loader = DataLoader(\n",
    "    pretrain_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # <--- Increased from 2 (Monitor resources)\n",
    "    pin_memory=True, # <--- Keep True for GPU\n",
    "    persistent_workers=True, # <--- Added for efficiency\n",
    "    prefetch_factor=2 # <--- Explicitly set default (can be tuned)\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimized pretraining DataLoader created:\")\n",
    "print(f\"  Dataset size: {len(pretrain_dataset)} samples.\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Num workers: {pretrain_loader.num_workers}\")\n",
    "print(f\"  Persistent workers: {pretrain_loader.persistent_workers}\")\n",
    "print(f\"  Pin memory: {pretrain_loader.pin_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(After Step 8: Create Datasets and DataLoaders - pretrain_loader exists)]\n",
    "\n",
    "# --- 8b. Create Validation Dataset and DataLoader ---\n",
    "\n",
    "# Use simpler transforms for validation (matching training's final size/normalization)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "# Reuse the same Dataset class (TinyImageNetPretrain is suitable)\n",
    "# Use the 'validation_files' list created in the modified Step 5\n",
    "val_dataset = TinyImageNetPretrain(\n",
    "    file_label_list=validation_files,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "if len(val_dataset) > 0:\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE * 2, # Often possible to use larger batch for validation (no grads)\n",
    "        shuffle=False, # No need to shuffle validation data\n",
    "        num_workers=2, # Can use fewer workers for validation if needed\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False # Less critical for validation\n",
    "    )\n",
    "    print(f\"\\nValidation DataLoader created:\")\n",
    "    print(f\"  Dataset size: {len(val_dataset)} samples.\")\n",
    "    print(f\"  Batch size: {val_loader.batch_size}\") # Print actual batch size used\n",
    "else:\n",
    "    print(\"\\nWarning: Validation dataset is empty. Skipping validation loader creation.\")\n",
    "    val_loader = None\n",
    "\n",
    "# [(Proceed to Step 9: Model Setup)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Model Setup (No changes needed) ---\n",
    "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
    "model = timm.create_model(\n",
    "    MODEL_NAME,\n",
    "    pretrained=False,\n",
    "    num_classes=num_id_classes\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded and moved to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. Training Setup (Optimizer, Scaler, and Scheduler) ---\n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# --- Initialize AMP GradScaler ---\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"AMP (Automatic Mixed Precision) enabled: {use_amp}\")\n",
    "\n",
    "# --- Initialize Learning Rate Scheduler ---\n",
    "# Calculate total training steps and warmup steps\n",
    "# Need len(pretrain_loader) which is calculated after DataLoader creation (Step 8)\n",
    "try:\n",
    "    steps_per_epoch = len(pretrain_loader)\n",
    "    total_training_steps = NUM_EPOCHS * steps_per_epoch\n",
    "    warmup_steps = WARMUP_EPOCHS * steps_per_epoch\n",
    "    cosine_steps = total_training_steps - warmup_steps\n",
    "\n",
    "    print(f\"\\nScheduler Configuration:\")\n",
    "    print(f\"  Total Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  Warmup Epochs: {WARMUP_EPOCHS}\")\n",
    "    print(f\"  Steps per Epoch: {steps_per_epoch}\")\n",
    "    print(f\"  Total Training Steps: {total_training_steps}\")\n",
    "    print(f\"  Warmup Steps: {warmup_steps}\")\n",
    "    print(f\"  Cosine Annealing Steps: {cosine_steps}\")\n",
    "    print(f\"  Peak LR: {LEARNING_RATE}\")\n",
    "\n",
    "    if warmup_steps > total_training_steps:\n",
    "         raise ValueError(\"WARMUP_EPOCHS cannot be greater than or equal to NUM_EPOCHS\")\n",
    "    if warmup_steps < 0:\n",
    "         raise ValueError(\"WARMUP_EPOCHS cannot be negative\")\n",
    "\n",
    "    # Scheduler 1: Linear Warmup\n",
    "    # Starts from LEARNING_RATE * start_factor and linearly increases to LEARNING_RATE\n",
    "    linear_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=0.01, # Start LR = 0.01 * LEARNING_RATE\n",
    "        end_factor=1.0,    # End LR = 1.0 * LEARNING_RATE\n",
    "        total_iters=warmup_steps # Number of steps for warmup\n",
    "    )\n",
    "\n",
    "    # Scheduler 2: Cosine Annealing Decay\n",
    "    # Starts annealing from LEARNING_RATE down to eta_min over cosine_steps\n",
    "    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=cosine_steps, # Number of steps for one cosine cycle (after warmup)\n",
    "        eta_min=1e-6        # Minimum learning rate\n",
    "    )\n",
    "\n",
    "    # Combine Schedulers: Use Linear for `warmup_steps`, then switch to Cosine\n",
    "    # Milestones are the steps at which to switch schedulers.\n",
    "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[linear_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_steps] # Switch to cosine_scheduler at step `warmup_steps`\n",
    "    )\n",
    "    print(\"  Using SequentialLR (Linear Warmup + Cosine Annealing).\")\n",
    "\n",
    "except NameError:\n",
    "     print(\"Error: pretrain_loader not defined before scheduler setup. Ensure DataLoader is created first.\")\n",
    "     scheduler = None # Or handle error appropriately\n",
    "except Exception as e:\n",
    "     print(f\"Error setting up scheduler: {e}\")\n",
    "     scheduler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(After Step 10: Training Setup)]\n",
    "\n",
    "# --- 10b. Validation Function ---\n",
    "def validate_one_epoch(model, loader, criterion, device, use_amp):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        progress_bar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Use autocast for consistency, although grads aren't computed\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            progress_bar.set_postfix(loss=loss.item(), batch_acc=(predicted == labels).float().mean().item())\n",
    "\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No samples processed during validation.\")\n",
    "        return 0.0, 0.0 # Return zeros if no samples\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 11. Training Loop (Optimized with AMP and Scheduler) ---\n",
    "print(f\"\\nStarting Optimized Pretraining w/ Scheduler for {NUM_EPOCHS} epochs...\")\n",
    "start_train_time = time.time()\n",
    "\n",
    "# Initialize history dictionary\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "# <<< ADDED: Early Stopping Initialization >>>\n",
    "epochs_no_improve = 0\n",
    "best_metric_value = None\n",
    "best_model_state = None # Store the best model state_dict here\n",
    "# Determine initial best value based on metric (minimize loss, maximize accuracy)\n",
    "if EARLY_STOPPING_METRIC == 'val_loss':\n",
    "    best_metric_value = float('inf')\n",
    "    print(f\"Early stopping monitoring '{EARLY_STOPPING_METRIC}' (lower is better). Patience: {PATIENCE}\")\n",
    "elif EARLY_STOPPING_METRIC == 'val_acc':\n",
    "    best_metric_value = float('-inf')\n",
    "    print(f\"Early stopping monitoring '{EARLY_STOPPING_METRIC}' (higher is better). Patience: {PATIENCE}\")\n",
    "else:\n",
    "    print(f\"Warning: Invalid EARLY_STOPPING_METRIC '{EARLY_STOPPING_METRIC}'. Disabling early stopping.\")\n",
    "    PATIENCE = float('inf') # Effectively disable stopping if metric is wrong\n",
    "# <<< END ADDED >>>\n",
    "\n",
    "if scheduler is None:\n",
    "     print(\"ERROR: Scheduler not initialized. Check Step 10.\")\n",
    "     # Decide how to proceed if scheduler failed\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode\n",
    "    train_running_loss = 0.0\n",
    "    train_correct_predictions = 0\n",
    "    train_total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(pretrain_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} Train\", leave=False)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(progress_bar):\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # --- Training Statistics ---\n",
    "        train_running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total_samples += labels.size(0)\n",
    "        train_correct_predictions += (predicted == labels).sum().item()\n",
    "        # --- End Statistics ---\n",
    "\n",
    "        if (i + 1) % 100 == 0 or (i + 1) == len(pretrain_loader): # Update less often\n",
    "             current_lr = optimizer.param_groups[0]['lr']\n",
    "             progress_bar.set_postfix(loss=loss.item(), batch_acc=(predicted == labels).float().mean().item(), lr=f\"{current_lr:.1e}\")\n",
    "\n",
    "    epoch_train_loss = train_running_loss / train_total_samples if train_total_samples > 0 else 0\n",
    "    epoch_train_acc = train_correct_predictions / train_total_samples if train_total_samples > 0 else 0\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    current_val_loss, current_val_acc = 0.0, 0.0 # Default values\n",
    "    if val_loader:\n",
    "      current_val_loss, current_val_acc = validate_one_epoch(model, val_loader, criterion, device, use_amp)\n",
    "\n",
    "    # --- Store History ---\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['train_acc'].append(epoch_train_acc * 100) # Store as percentage\n",
    "    history['val_loss'].append(current_val_loss)\n",
    "    history['val_acc'].append(current_val_acc * 100) # Store as percentage\n",
    "    history['lr'].append(current_lr)\n",
    "\n",
    "    # --- Print Epoch Results ---\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {current_val_loss:.4f}, Val Acc: {current_val_acc:.4f} | \"\n",
    "          f\"LR: {current_lr:.1e}\")\n",
    "\n",
    "    # --- Optional: Plot progress during training (can be noisy) ---\n",
    "    # if (epoch + 1) % 5 == 0: # Plot every 5 epochs\n",
    "    #    plot_training_progress(history, \"TinyImageNet_Pretrain_InProgress\")\n",
    "   # <<< ADDED: Early Stopping Check >>>\n",
    "    if val_loader and PATIENCE != float('inf'): # Only check if validation ran and patience is set\n",
    "        # Determine the metric to check\n",
    "        metric_to_check = current_val_loss if EARLY_STOPPING_METRIC == 'val_loss' else current_val_acc * 100 # Use % acc\n",
    "\n",
    "        improved = False\n",
    "        if EARLY_STOPPING_METRIC == 'val_loss':\n",
    "            # Check if current loss is lower than best loss by MIN_DELTA\n",
    "            if metric_to_check < best_metric_value - abs(MIN_DELTA): # Ensure MIN_DELTA is positive for loss check\n",
    "                 improved = True\n",
    "        elif EARLY_STOPPING_METRIC == 'val_acc':\n",
    "            # Check if current accuracy is higher than best accuracy by MIN_DELTA\n",
    "             if metric_to_check > best_metric_value + abs(MIN_DELTA): # Ensure MIN_DELTA is positive for acc check\n",
    "                 improved = True\n",
    "\n",
    "        if improved:\n",
    "            print(f\"  ({EARLY_STOPPING_METRIC} improved from {best_metric_value:.4f} to {metric_to_check:.4f})\")\n",
    "            best_metric_value = metric_to_check\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model state in memory\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  ({EARLY_STOPPING_METRIC} did not improve from {best_metric_value:.4f}. Patience: {epochs_no_improve}/{PATIENCE})\")\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n",
    "            break # Exit the training loop\n",
    "    # <<< END ADDED >>>\n",
    "\n",
    "end_train_time = time.time()\n",
    "print(f\"\\nOptimized pretraining w/ Scheduler & Validation finished in {(end_train_time - start_train_time)/60:.2f} minutes.\")\n",
    "\n",
    "# --- Output Final Accuracy ---\n",
    "final_train_acc = history['train_acc'][-1] if history['train_acc'] else 0\n",
    "final_val_acc = history['val_acc'][-1] if history['val_acc'] else 0\n",
    "print(f\"\\nFinal Training Accuracy: {final_train_acc:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "\n",
    "# --- (Proceed to Step 12: Save Results) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 12. Save Results ---\n",
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# a) Save Model State Dictionary\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "  os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "pretrain_files_onlypaths = [item[0] for item in pretrain_files] # Save only paths\n",
    "\n",
    "model_info = {\n",
    "  'model_state_dict': model.state_dict(),\n",
    "  'class_info': {\n",
    "    'ind_all_ids': id_wnids, # All the string class IDs of IND data\n",
    "    'ind_pretrain_files_paths': pretrain_files_onlypaths,\n",
    "    'ind_reserved_files_paths': id_reserved_files_with_wnid,\n",
    "    'ood_files_paths': ood_class_files,\n",
    "    'ood_class_ids': ood_wnids, # All the string class IDs of IND data\n",
    "    'class_id_to_pretrain_label': wnid_to_pretrain_label,\n",
    "    'wnid_to_name': wnid_to_name\n",
    "  }\n",
    "}\n",
    "\n",
    "model_save_path = os.path.join(SAVE_DIR, f\"{MODEL_NAME}_tinyimagenet_pretrained.pth\")\n",
    "torch.save(model_info, model_save_path)\n",
    "print(f\"All Model data saved to: {model_save_path}\")\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Pretrained Model: {MODEL_NAME}\")\n",
    "print(f\"Total Classes: {num_total_classes}\")\n",
    "print(f\"ID Classes ({len(id_wnids)}): Used for pretraining (subset) and ID detection examples.\")\n",
    "print(f\"OOD Classes ({len(ood_wnids)}): Held out entirely, used for OOD detection examples.\")\n",
    "print(f\"Pretraining Examples: {len(pretrain_files_onlypaths)} (from {len(id_wnids)} classes)\")\n",
    "print(f\"ID-Reserved Examples: {len(id_reserved_files_with_wnid)} (from {len(id_wnids)} classes, for later OOD detection)\")\n",
    "print(f\"OOD-Class Examples: {len(ood_class_files)} (from {len(ood_wnids)} classes, for later OOD detection)\")\n",
    "print(f\"Saved artifacts directory: {SAVE_DIR}\")\n",
    "print(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omccood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
